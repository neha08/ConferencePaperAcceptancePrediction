# Conference Paper Acceptance Prediction (Acceptometer)

Academic research papers have several integral and peripheral aspects apart from the core content such as the authorsâ€™ professional affiliations, the number of references or the text readability, to name a few. While the methods and ideas presented in the core content are the most crucial and driving factor for the paper to be accepted by peer reviewers to a conference, it is possible that the aforementioned non-core aspects also influence or contribute to the decision unknowingly. It is possible that the presence of these factors in a certain measure is a subtle indication of a high-quality research paper. For example, it is possible that because the author belongs to one of the topmost universities, she already has a good history of accepted research papers, and her next research paper will have highly innovative ideas articulated well and thus stand a good chance of acceptance.

This project explores this idea and aims to assess if these non-core-content factors could play a role in predicting the chances of a paper being accepted to a conference. It must be noted that it is not intended to say that peer-reviewers get biased or make decisions because of the presence of these factors. The hypothesis is that research papers, potentially acceptable because of their merit, may have these factors in common.

In order for the tool to have more capabilities for recommendation than relying on non-core factors, unsupervised methods are also used to find patterns in the data and explore its underlying structure, through tasks like topic-modelling and clustering. This could help in detecting topic and domain trends over conferences over several years and or in ascertaining peculiarities of papers potentially acceptable papers.

The dataset is downloaded and parsed using PeerReed which is a dataset of scientific peer reviews available to help researchers study this important artifact. The dataset consists of 14.7K paper drafts and the corresponding accept/reject decisions in top-tier venues including ACL, NIPS and ICLR. The dataset also includes 10.7K textual peer reviews written by experts for a subset of the papers. By parsing the JSONs we constructed the different features based on different parts of the paper (eg. abstract, references) for our supervised learning dataset. The labels were booleans for accepted or rejected decisions.

The overall accuracy is ~65 which is not very higher than that of the baseline in PeerRead. However, given that our data is trained only for ICLR 2017 and the baseline also includes arxiv papers, it would not be a fair comparison. While the baseline model algorithms are not known, our Random Forest model without dimension reduction gave the best accuracy, which shows that the feature engineering we performed especially by adding our own new features definitely helped. It is possible that our model did not show a significant improvement on the baseline because of the small amount of data available to train and test on.